{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SO Expertise Model Evaluation - optimizer.ipynb","provenance":[{"file_id":"1_PPdIMlPIhQvYncVmFxDW0bVkaTpOoOw","timestamp":1574107135190},{"file_id":"1Xcnk0Xpo8XF_0cwCmRCp2iy1F3zo5DNP","timestamp":1573854196095}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Qeq6XHToZ0sH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"db7fc44a-8b6d-4404-c1f6-8c38f2381641","executionInfo":{"status":"ok","timestamp":1576697245584,"user_tz":300,"elapsed":28198,"user":{"displayName":"utherpendragon17@yahoo.com","photoUrl":"","userId":"00837845241358288631"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_KHAL52iGNG-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":653},"outputId":"50751ea3-ff96-427a-c00e-53f3ce6c833e","executionInfo":{"status":"ok","timestamp":1576697256883,"user_tz":300,"elapsed":7904,"user":{"displayName":"utherpendragon17@yahoo.com","photoUrl":"","userId":"00837845241358288631"}}},"source":["!pip install pyLDAvis"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pyLDAvis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.6)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.17.4)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.3.3)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.14.1)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Collecting funcy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 37.1MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.6.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.0.2)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (42.0.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n","Building wheels for collected packages: pyLDAvis, funcy\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=ff0e83ae1fe829c0c02e9c0f219bfbd9677b1d2453243abc214ec0d4eab484ec\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32040 sha256=35b0b761398d0356d5adba6d344d9ef5d302f1e16f5a00b84003b2f4eecb58f1\n","  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n","Successfully built pyLDAvis funcy\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2mFr7sLUH995","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"45638d52-5664-4573-fbf1-ea04c25e17cb","executionInfo":{"status":"ok","timestamp":1576697265684,"user_tz":300,"elapsed":3901,"user":{"displayName":"utherpendragon17@yahoo.com","photoUrl":"","userId":"00837845241358288631"}}},"source":["!pip install scikit-optimize"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting scikit-optimize\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n","\r\u001b[K     |████▍                           | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.1MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.17.4)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.14.1)\n","Installing collected packages: scikit-optimize\n","Successfully installed scikit-optimize-0.5.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GEmRsAw3ZozG","colab_type":"code","colab":{}},"source":["from sklearn.metrics import precision_recall_fscore_support\n","from gensim.models.keyedvectors import KeyedVectors\n","from gensim.parsing.preprocessing import remove_stopwords, strip_numeric, preprocess_string\n","import numpy as np\n","import pandas as pd\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import pickle\n","import pyLDAvis.gensim\n","from joblib import Parallel, delayed, cpu_count\n","import warnings\n","warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.matutils import jaccard_distance\n","from joblib import Parallel, delayed, cpu_count\n","from gensim.sklearn_api import LdaTransformer\n","from gensim.models import CoherenceModel, LdaModel\n","from gensim.corpora.mmcorpus import MmCorpus\n","from gensim.test.utils import datapath\n","from gensim.corpora.dictionary import Dictionary\n","from skopt import dump\n","from skopt import gp_minimize\n","from skopt.utils import use_named_args\n","import skopt\n","import csv\n","import time\n","import statistics"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQqqphp3kb9q","colab_type":"text"},"source":["# Load in human annotations"]},{"cell_type":"code","metadata":{"id":"n38me07Nj6DD","colab_type":"code","colab":{}},"source":["def intersect(a, b):\n","    \"\"\" return the intersection of two lists \"\"\"\n","    return list(set(a) & set(b))\n","\n","def union(a, b):\n","    \"\"\" return the union of two lists \"\"\"\n","    return list(set(a) | set(b))\n","\n","def load_annotations():\n","  SO_annotation = pd.read_csv('/content/drive/My Drive/Expertise Experiments/SO_annotations_processed.csv', header = 0,\n","                        names = [\"sample_ID\",\"profile_url\",\"unified_Id\",\"internal_ID\",\"Annotator_1\",\"Annotator_2\", \"Processed_Annotator_1\", \"Processed_Annotator_2\"])\n","\n","  GH_annotation = pd.read_csv('/content/drive/My Drive/Expertise Experiments/GH_annotations_processed.csv', header = 0, \n","                        names = [\"sample_ID\",\"profile_url\",\"unified_Id\",\"internal_ID\",\"Annotator_1\",\"Annotator_2\", \"Processed_Annotator_1\", \"Processed_Annotator_2\"])\n","  \n","  GH_IDs = GH_annotation[\"internal_ID\"]\n","  SO_IDs = SO_annotation[\"internal_ID\"]\n","\n","  GH_annotation_intersect = {}\n","  GH_annotation_union = {}\n","  SO_annotation_intersect = {}\n","  SO_annotation_union = {}\n","\n","  for index, row in SO_annotation.iterrows():\n","    a1 = row['Processed_Annotator_1'].split(\";\")\n","    a2 = row['Processed_Annotator_2'].split(\";\")\n","\n","    if '' in a1:\n","      a1.remove('')\n","    if '' in a2:\n","      a2.remove('')\n","\n","    SO_annotation_intersect[row['internal_ID']] = intersect(a1, a2)\n","    SO_annotation_union[row['internal_ID']] = union(a1, a2)\n","\n","  for index, row in GH_annotation.iterrows():\n","    a1 = row['Processed_Annotator_1'].split(\";\")\n","    a2 = row['Processed_Annotator_2'].split(\";\")\n","\n","    if '' in a1:\n","      a1.remove('')\n","    if '' in a2:\n","      a2.remove('')\n","\n","    GH_annotation_intersect[row['internal_ID']] = intersect(a1, a2)\n","    GH_annotation_union[row['internal_ID']] = union(a1, a2)\n","\n","  return GH_IDs, SO_IDs, GH_annotation_intersect, GH_annotation_union, SO_annotation_intersect, SO_annotation_union"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4LuqtD606Ny","colab_type":"text"},"source":["# Create topic info data frame by calling get_topic_info(lda_model, corpus, dictionary)"]},{"cell_type":"code","metadata":{"id":"0U4UbY_x05va","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","import funcy as fp\n","import numpy as np\n","from scipy.sparse import issparse\n","\n","def get_topic_info(topic_model, corpus, dictionary, doc_topic_dist=None):\n","  opts = fp.merge(pyLDAvis_prepare(topic_model, corpus, dictionary, doc_topic_dist))\n","  return my_prepare(**opts)\n","  \n","def _chunks(l, n):\n","    \"\"\" Yield successive n-sized chunks from l.\n","    \"\"\"\n","    for i in range(0, len(l), n):\n","        yield l[i:i + n]\n","\n","\n","def _job_chunks(l, n_jobs):\n","    n_chunks = n_jobs\n","    if n_jobs < 0:\n","        # so, have n chunks if we are using all n cores/cpus\n","        n_chunks = cpu_count() + 1 - n_jobs\n","\n","    return _chunks(l, n_chunks)\n","\n","\n","def _find_relevance(log_ttd, log_lift, R, lambda_):\n","    relevance = lambda_ * log_ttd + (1 - lambda_) * log_lift\n","    return relevance.T.apply(lambda s: s.sort_values(ascending=False).index).head(R)\n","\n","\n","def _find_relevance_chunks(log_ttd, log_lift, R, lambda_seq):\n","    return pd.concat([_find_relevance(log_ttd, log_lift, R, l) for l in lambda_seq])\n","\n","\n","\n","def _df_with_names(data, index_name, columns_name):\n","    if type(data) == pd.DataFrame:\n","        # we want our index to be numbered\n","        df = pd.DataFrame(data.values)\n","    else:\n","        df = pd.DataFrame(data)\n","    df.index.name = index_name\n","    df.columns.name = columns_name\n","    return df\n","\n","\n","def _series_with_name(data, name):\n","    if type(data) == pd.Series:\n","        data.name = name\n","        # ensures a numeric index\n","        return data.reset_index()[name]\n","    else:\n","        return pd.Series(data, name=name)\n","\n","\n","def _topic_info(topic_term_dists, topic_proportion, term_frequency, term_topic_freq,\n","                vocab, lambda_step, R, n_jobs):\n","    # marginal distribution over terms (width of blue bars)\n","    term_proportion = term_frequency / term_frequency.sum()\n","\n","    # compute the distinctiveness and saliency of the terms:\n","    # this determines the R terms that are displayed when no topic is selected\n","    topic_given_term = topic_term_dists / topic_term_dists.sum()\n","    kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n","    distinctiveness = kernel.sum()\n","    saliency = term_proportion * distinctiveness\n","    # Order the terms for the \"default\" view by decreasing saliency:\n","    default_term_info = pd.DataFrame({\n","        'saliency': saliency,\n","        'Term': vocab,\n","        'Freq': term_frequency,\n","        'Total': term_frequency,\n","        'Category': 'Default'})\n","    default_term_info = default_term_info.sort_values(\n","        by='saliency', ascending=False).head(R).drop('saliency', 1)\n","    # Rounding Freq and Total to integer values to match LDAvis code:\n","    default_term_info['Freq'] = np.floor(default_term_info['Freq'])\n","    default_term_info['Total'] = np.floor(default_term_info['Total'])\n","    ranks = np.arange(R, 0, -1)\n","    default_term_info['logprob'] = default_term_info['loglift'] = ranks\n","\n","    # compute relevance and top terms for each topic\n","    log_lift = np.log(topic_term_dists / term_proportion)\n","    log_ttd = np.log(topic_term_dists)\n","    lambda_seq = np.arange(0, 1 + lambda_step, lambda_step)\n","\n","    def topic_top_term_df(tup):\n","        new_topic_id, (original_topic_id, topic_terms) = tup\n","        term_ix = topic_terms.unique()\n","        return pd.DataFrame({'Term': vocab[term_ix],\n","                             'Freq': term_topic_freq.loc[original_topic_id, term_ix],\n","                             'Total': term_frequency[term_ix],\n","                             'logprob': log_ttd.loc[original_topic_id, term_ix].round(4),\n","                             'loglift': log_lift.loc[original_topic_id, term_ix].round(4),\n","                             'Category': 'Topic%d' % new_topic_id})\n","\n","    top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n","                          (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n","                          for ls in _job_chunks(lambda_seq, n_jobs)))\n","    topic_dfs = map(topic_top_term_df, enumerate(top_terms.T.iterrows(), 1))\n","    return pd.concat([default_term_info] + list(topic_dfs), sort=True)\n","\n","def pyLDAvis_prepare(topic_model, corpus, dictionary, doc_topic_dists=None):\n","    import gensim\n","    if not gensim.matutils.ismatrix(corpus):\n","        corpus_csc = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary))\n","    else:\n","        corpus_csc = corpus\n","        # Need corpus to be a streaming gensim list corpus for len and inference functions below:\n","        corpus = gensim.matutils.Sparse2Corpus(corpus_csc)\n","\n","    vocab = list(dictionary.token2id.keys())\n","    # TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\n","    # for now, I'll just make sure we don't ever get zeros...\n","    beta = 0.01\n","    fnames_argsort = np.asarray(list(dictionary.token2id.values()), dtype=np.int_)\n","    term_freqs = corpus_csc.sum(axis=1).A.ravel()[fnames_argsort]\n","    term_freqs[term_freqs == 0] = beta\n","    doc_lengths = corpus_csc.sum(axis=0).A.ravel()\n","\n","    if hasattr(topic_model, 'lda_alpha'):\n","        num_topics = len(topic_model.lda_alpha)\n","    else:\n","        num_topics = topic_model.num_topics\n","\n","    if doc_topic_dists is None:\n","        # If its an HDP model.\n","        if hasattr(topic_model, 'lda_beta'):\n","            gamma = topic_model.inference(corpus)\n","        else:\n","            gamma, _ = topic_model.inference(corpus)\n","        doc_topic_dists = gamma / gamma.sum(axis=1)[:, None]\n","    else:\n","        if isinstance(doc_topic_dists, list):\n","            #doc_topic_dists = np.matrix(gensim.matutils.corpus2dense(doc_topic_dists, num_topics).T)\n","            doc_topic_dists = gensim.matutils.corpus2dense(doc_topic_dists, num_topics).T\n","        elif issparse(doc_topic_dists):\n","            doc_topic_dists = doc_topic_dists.T.todense()\n","        doc_topic_dists = doc_topic_dists / doc_topic_dists.sum(axis=1)\n","\n","    # get the topic-term distribution straight from gensim without\n","    # iterating over tuples\n","    if hasattr(topic_model, 'lda_beta'):\n","        topic = topic_model.lda_beta\n","    else:\n","        topic = topic_model.state.get_lambda()\n","    topic = topic / topic.sum(axis=1)[:, None]\n","    topic_term_dists = topic[:, fnames_argsort]\n","\n","    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n","    topic_proportion = (topic_freq / topic_freq.sum())\n","\n","    term_topic_freq = (topic_term_dists.T * topic_freq).T\n","\n","    return {'topic_term_dists': topic_term_dists, 'doc_topic_dists': doc_topic_dists, \n","            'doc_lengths': doc_lengths, 'vocab': vocab, 'term_frequency': term_freqs}\n","\n","def my_prepare(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency,\n","            R=30, lambda_step=0.01, n_jobs=-1, sort_topics=True):\n","\n","    topic_term_dists = _df_with_names(topic_term_dists, 'topic', 'term')\n","    doc_topic_dists = _df_with_names(doc_topic_dists, 'doc', 'topic')\n","    term_frequency = _series_with_name(term_frequency, 'term_frequency')\n","    doc_lengths = _series_with_name(doc_lengths, 'doc_length')\n","    vocab = _series_with_name(vocab, 'vocab')\n","\n","    topic_freq = (doc_topic_dists.T * doc_lengths).T.sum()\n","    if (sort_topics):\n","        topic_proportion = (topic_freq / topic_freq.sum()).sort_values(ascending=False)\n","    else:\n","        topic_proportion = (topic_freq / topic_freq.sum())\n","\n","    topic_order = topic_proportion.index\n","    # reorder all data based on new ordering of topics\n","    topic_freq = topic_freq[topic_order]\n","    topic_term_dists = topic_term_dists.iloc[topic_order]\n","    doc_topic_dists = doc_topic_dists[topic_order]\n","\n","    # token counts for each term-topic combination (widths of red bars)\n","    term_topic_freq = (topic_term_dists.T * topic_freq).T\n","    term_frequency = np.sum(term_topic_freq, axis=0)\n","\n","    topic_info = _topic_info(topic_term_dists, topic_proportion,\n","                             term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n","    return topic_info"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXE-N8gLkjSA","colab_type":"text"},"source":["# Define techniques"]},{"cell_type":"markdown","metadata":{"id":"JzBTy2rhg-73","colab_type":"text"},"source":["## Word2Vec user and topic Embeddings using Max and Avg pooling"]},{"cell_type":"code","metadata":{"id":"E6DIAhpeg_Y6","colab_type":"code","colab":{}},"source":["def run_Word2Vec_emb(lda, threshold, maxPool):\n","  user_vectors = get_user_emb()\n","  avg_topic_vectors, max_topic_vectors = get_topic_emb(lda)\n","  \n","  if maxPool:\n","    topic_vectors = max_topic_vectors\n","  else:\n","    topic_vectors = avg_topic_vectors\n","  \n","  cos_sim = cosine_similarity(user_vectors, topic_vectors)\n","  user_topic_mapping = create_user_topic_mapping(cos_sim, threshold)\n","  return user_topic_mapping"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Og1C2dLe_BSp","colab_type":"text"},"source":["Create user and topic embeddings using SO_Word2Vec_200"]},{"cell_type":"code","metadata":{"id":"f148KikG-wKS","colab_type":"code","colab":{}},"source":["word_vectors = KeyedVectors.load(\"/content/drive/My Drive/embeddings/SO_pre-trained_vectors.kv\", mmap='r')\n","\n","CUSTOM_FILTERS = [lambda x: strip_numeric, remove_stopwords]\n","\n","def word2vec_embedding_lookup(words):\n","  vectors = []\n","  for w in words:\n","    try:\n","      vec = word_vectors[w]\n","      vectors.append(vec)\n","    except:\n","      try:\n","        w_transformed = w.replace(\".\", \"\").replace(\"=\", \"\").replace(\"-\", \"\").replace(\"*\", \"\").replace(\"'\", \"\").replace(\"`\", \"\").replace(\"|\", \"\").replace('\\\\', \"\").replace(\"/\", \"\").replace(\"$\", \"\").replace(\"^\", \"\").replace(\"#\", \"\").replace(\"&\", \"\").replace(\"@\", \"\")\n","        vec = word_vectors[w_transformed]\n","        vectors.append(vec)\n","      except:\n","        try:\n","          w_stripped = preprocess_string(w_transformed, CUSTOM_FILTERS)\n","          vec = word_vectors[w_stripped]\n","          vectors.append(vec)\n","        except:\n","          continue\n","  return np.array(vectors)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQXa0Hja8l5l","colab_type":"text"},"source":["Get topic and user emb from pre-trained Word2Vec model"]},{"cell_type":"code","metadata":{"id":"WBK3X3J_8h6Q","colab_type":"code","colab":{}},"source":["def get_user_emb():\n","  user_embeddings = []\n","  for i in range(0,len(texts)):\n","    word_vectors = word2vec_embedding_lookup(list(set(texts[i]).intersection(terms)))\n","    try:\n","      feature_vector = np.max(word_vectors, axis=0)\n","      user_embeddings.append(feature_vector)\n","    except ValueError:\n","      user_embeddings.append(np.zeros((200,)))  # 200 x 1 vector of 0's, since the word2vec model is 200 dimensional\n","  return np.array(user_embeddings)\n","\n","def get_topic_emb(lda):\n","  avg_topic_emb = []\n","  max_topic_emb = []\n","  number_of_topicWords = 20\n","  topic_num = lda.num_topics \n","\n","  for topic in range(0, topic_num):  # for each topic inside a specific model\n","    results = lda.show_topic(topic, topn=number_of_topicWords)\n","\n","    topic_words = []\n","    for i in range(0,number_of_topicWords): # for each topic word inside a topic\n","      topic_words.append(results[i][0])\n","\n","    word_vectors = word2vec_embedding_lookup(topic_words)\n","    avg_feature_vector = np.average(word_vectors, axis=0)\n","    max_feature_vector = np.max(word_vectors, axis=0)\n","\n","    avg_topic_emb.append(avg_feature_vector)\n","    max_topic_emb.append(max_feature_vector)\n","  return np.asarray(avg_topic_emb), np.asarray(max_topic_emb)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uiFcMLRuAM0","colab_type":"text"},"source":["## LDA_topicEmbedding using Max-pooling and Avg-pooling\n","\n"]},{"cell_type":"code","metadata":{"id":"4H2OF-ZohOCx","colab_type":"code","colab":{}},"source":["def embedding_lookup(term_embeddings, word):\n","  return np.array(term_embeddings[word])\n","\n","def get_LDA_user_emb(topic_num, term_emb):\n","  user_embeddings = []\n","  for i in range(0,len(texts)):\n","    word_vectors = embedding_lookup(term_emb, list(set(texts[i]).intersection(terms)))\n","    try:\n","      feature_vector = np.max(word_vectors, axis=1)\n","      user_embeddings.append(feature_vector)\n","    except ValueError:\n","      user_embeddings.append(np.zeros((topic_num,)))\n","  return np.array(user_embeddings)\n","\n","def get_LDA_topic_emb(lda_model, term_emb):\n","  avg_topic_emb = []\n","  max_topic_emb = []\n","  number_of_topicWords = 20\n","\n","  for topic in range(0, lda_model.num_topics):  # for each topic inside a specific model\n","    results = lda_model.show_topic(topic, topn=number_of_topicWords)\n","\n","    topic_words = []\n","    for i in range(0,number_of_topicWords):\n","      topic_words.append(results[i][0])\n","\n","    word_vectors = embedding_lookup(term_emb, topic_words)\n","    avg_feature_vector = np.average(word_vectors, axis=1)\n","    max_feature_vector = np.max(word_vectors, axis=1)\n","\n","    avg_topic_emb.append(avg_feature_vector)\n","    max_topic_emb.append(max_feature_vector)\n","\n","  avg_topic_vectors = np.array(avg_topic_emb)\n","  max_topic_vectors = np.array(max_topic_emb)\n","  return avg_topic_vectors, max_topic_vectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORmTvQfzwE-U","colab_type":"code","colab":{}},"source":["def jaccard_sim(A, B):\n","  return 1 - jaccard_distance(set(A), set(B))     # jaccard sim = 1 - jaccard distance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_u2biiiZ_VMi","colab_type":"code","colab":{}},"source":["def run_LDA_emb(lda_model, term_emb, threshold, maxPool):\n","  user_vectors = get_LDA_user_emb(lda_model.num_topics, term_emb)\n","  avg_topic_vectors, max_topic_vectors = get_LDA_topic_emb(lda_model, term_emb)\n","\n","  if maxPool:\n","    topic_vectors = max_topic_vectors\n","  else:\n","    topic_vectors = avg_topic_vectors\n","\n","  cos_sim = cosine_similarity(user_vectors, topic_vectors)\n","  user_topic_mapping = create_user_topic_mapping(cos_sim, threshold)\n","  return user_topic_mapping"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUGusUdV0BCh","colab_type":"code","colab":{}},"source":["def create_user_topic_mapping(cos_sims, threshold):\n","  user_topic_mapping = {}\n","  for user_i in range(0, 83550):   # counting for users 0 --> 83549\n","    user_topic_mapping[user_i] = ['Topic' + str(index+1) for index, value in enumerate(cos_sims[user_i]) if value > threshold]\n","  return user_topic_mapping"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHJrLHi57Rir","colab_type":"code","colab":{}},"source":["def get_user_expertise(topicInfo, user_i, topic_terms = 20):\n","  optimal_lambda_val = 0.6\n","  expertise = get_relevant_terms(topicInfo, user_i, optimal_lambda_val, topic_terms)\n","  return expertise"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gzi1dWjPMXTc","colab_type":"code","colab":{}},"source":["def get_relevant_terms(topic_info, topics, _lambda, term_num):\n","  \"\"\"Retuns a list of top-n keywords (where n = term_num) that have the highest relevance score for the topics the the user is in.\"\"\"\n","\n","  tdf = pd.DataFrame(topic_info[topic_info.Category.isin(topics)])\n","  stdf = tdf.assign(relevance=_lambda * tdf['logprob'] + (1 - _lambda) * tdf['loglift'])\n","  new_df = stdf.sort_values('relevance', ascending=False)\n","\n","  term_list = new_df['Term'].tolist()\n","  if '-PRON-' in term_list:\n","    term_list.remove('-PRON-')\n","  if ' ' in term_list:\n","    term_list.remove(' ')\n","\n","  if term_num > len(term_list):\n","    return term_list\n","  else:\n","    return term_list[0:term_num]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C28HHVbQ7Kyu","colab_type":"text"},"source":["## LDA Topic Distribution based Expertise"]},{"cell_type":"code","metadata":{"id":"AXddXFzUlpb1","colab_type":"code","colab":{}},"source":["def LDA_topicDistr(lda_model, topicInfo, user_i, threshold, topic_terms):\n","  user_topic_membership = create_user_topic_thresholding(lda_model, user_i, threshold)\n","  user_expertise = get_expertise_for_user_i(topicInfo, user_topic_membership, topic_terms)\n","  return user_expertise"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFFFAX0ZJXFK","colab_type":"code","colab":{}},"source":["def create_user_topic_thresholding(lda_model, user_i, threshold):\n","  user_i_topic_distr = lda_model.get_document_topics(bow = corpus[user_i], minimum_probability = threshold)\n","\n","  topic_memberships = []\n","  for topic in user_i_topic_distr:\n","    topic_memberships.append('Topic' + str(topic[0]+1))   # topics are 0 to k-1, so offset by 1, since pyLDAvis indexes from 1 to k\n","\n","  return topic_memberships"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y__yRiy2Jogs","colab":{}},"source":["def get_expertise_for_user_i(topicInfo, user_topic_membership, topic_terms):\n","  optimal_lambda_val = 0.6\n","  expertise = get_relevant_terms(topicInfo, user_topic_membership, optimal_lambda_val, topic_terms)\n","  return expertise"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wWyIufXns5tb","colab_type":"text"},"source":["# Create Evaluation functions"]},{"cell_type":"code","metadata":{"id":"jmcOWxWyhuG8","colab_type":"code","colab":{}},"source":["def getExistingWordsFromModel(words):\n","  \"\"\" Checks if a list of words are in the dictionary of the word2vec model \"\"\"\n","  CUSTOM_FILTERS = [lambda x: strip_numeric, remove_stopwords]\n","  res = []\n","  for w in words:\n","    try:\n","      vec = word_vectors[w]\n","      res.append(w)\n","    except:\n","      try:\n","        w_transformed = w.replace(\".\", \"\").replace(\"=\", \"\").replace(\"-\", \"\").replace(\"*\", \"\").replace(\"'\", \"\").replace(\"`\", \"\").replace(\"|\", \"\").replace('\\\\', \"\").replace(\"/\", \"\").replace(\"$\", \"\").replace(\"^\", \"\").replace(\"&\", \"\").replace(\"@\", \"\").replace(\"%\", \"\")\n","        vec = word_vectors[w_transformed]\n","        res.append(w_transformed)\n","      except:\n","         try:\n","          w_stripped = preprocess_string(w_transformed, CUSTOM_FILTERS)\n","          vec = word_vectors[w_stripped]\n","          res.append(w_stripped)\n","         except:\n","           continue\n","  return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDSrPIlQo1_M","colab_type":"code","colab":{}},"source":["def evaluate_LDA_topicDistr(lda_model, topicInfo, threshold_t):\n","  BLEU_scores = []\n","  jacc_similarity = []\n","  cos_similarity = []\n","\n","  recall = []\n","  precision = []\n","  fscore = []\n","\n","  for user_i in SO_IDs:\n","    # or if you want intersect, use SO_annotation_intersect\n","    annotation = SO_annotation_union[user_i]\n","\n","    model_hypothesis = LDA_topicDistr(lda_model, topicInfo, user_i, threshold_t, topic_terms = len(annotation))\n","\n","    # 1-gram individual BLEU with smoothing function\n","    smooth = SmoothingFunction()\n","    BLEU_score = sentence_bleu(references = [annotation], hypothesis = model_hypothesis, weights = (1, 0, 0, 0), smoothing_function = smooth.method1)\n","    BLEU_scores.append(BLEU_score)\n","\n","    # calculate precision, recall, fscore\n","    if len(annotation) == len(model_hypothesis):\n","      micro_res = precision_recall_fscore_support(y_true = np.array(annotation), y_pred = np.array(model_hypothesis), average='micro')\n","      precision.append(micro_res[0])\n","      recall.append(micro_res[1])\n","      fscore.append(micro_res[2])\n","\n","    # calculate Jaccard similarity between annotation and model hypothesis \n","    jaccard_simm = jaccard_sim(annotation, model_hypothesis)\n","    jacc_similarity.append(jaccard_simm)\n","\n","    # Compute cosine similarity between annotation and model hypothesis \n","    a = getExistingWordsFromModel(annotation)\n","    b = getExistingWordsFromModel(model_hypothesis)\n","\n","    if len(a) > 0 and len(b) > 0:\n","      cos_sim = word_vectors.n_similarity(a, b)\n","      cos_similarity.append(cos_sim)\n","\n","  return BLEU_scores, jacc_similarity, cos_similarity, precision, recall, fscore"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDmycayRsXR7","colab_type":"code","colab":{}},"source":["def evaluate_LDA_topicEmb(lda_model, topicInfo, term_emb, threshold, maxPool):\n","  user_topic_mapping = run_LDA_emb(lda_model, term_emb, threshold, maxPool)\n","\n","  BLEU_scores = []\n","  jacc_similarity = []\n","  cos_similarity = []\n","\n","  recall = []\n","  precision = []\n","  fscore = []\n","\n","  for user_i in SO_IDs:\n","    # or if you want intersect, use SO_annotation_intersect\n","    annotation = SO_annotation_union[user_i]\n","\n","    model_hypothesis = get_user_expertise(topicInfo, user_topic_mapping[user_i], topic_terms = len(annotation))\n","\n","    # 1-gram individual BLEU with smoothing function\n","    smooth = SmoothingFunction()\n","    BLEU_score = sentence_bleu(references = [annotation], hypothesis = model_hypothesis, \n","                               weights = (1, 0, 0, 0), smoothing_function = smooth.method1)\n","    BLEU_scores.append(BLEU_score)\n","\n","    # calculate precision, recall, fscore\n","    if len(annotation) == len(model_hypothesis):\n","      micro_res = precision_recall_fscore_support(y_true = np.array(annotation), y_pred = np.array(model_hypothesis), average='micro')\n","      precision.append(micro_res[0])\n","      recall.append(micro_res[1])\n","      fscore.append(micro_res[2])\n","\n","    # calculate Jaccard similarity between annotation and model hypothesis \n","    jaccard_simm = jaccard_sim(annotation, model_hypothesis)\n","    jacc_similarity.append(jaccard_simm)\n","\n","    # Compute cosine similarity between annotation and model hypothesis \n","    a = getExistingWordsFromModel(annotation)\n","    b = getExistingWordsFromModel(model_hypothesis)\n","    \n","    if len(a) > 0 and len(b) > 0:\n","      cos_sim = word_vectors.n_similarity(a, b)\n","      cos_similarity.append(cos_sim)\n","\n","  return BLEU_scores, jacc_similarity, cos_similarity, precision, recall, fscore"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CGdpD00yh4Jy","colab_type":"code","colab":{}},"source":["def evaluate_Word2Vec_Emb(lda, topicInfo, threshold, maxPool):\n","  user_topic_mapping = run_Word2Vec_emb(lda, threshold, maxPool)\n","\n","  BLEU_scores = []\n","  jacc_similarity = []\n","  cos_similarity = []\n","\n","  recall = []\n","  precision = []\n","  fscore = []\n","\n","  for user_i in SO_IDs:\n","    # or if you want intersect, use SO_annotation_intersect\n","    annotation = SO_annotation_union[user_i]\n","    model_hypothesis = get_user_expertise(topicInfo, user_topic_mapping[user_i], topic_terms = len(annotation))\n","\n","    # 1-gram individual BLEU with smoothing function\n","    smooth = SmoothingFunction()\n","    BLEU_score = sentence_bleu(references = [annotation], hypothesis = model_hypothesis, \n","                               weights = (1, 0, 0, 0), smoothing_function = smooth.method1)\n","    BLEU_scores.append(BLEU_score)\n","\n","    # calculate precision, recall, fscore\n","    if len(annotation) == len(model_hypothesis):\n","      micro_res = precision_recall_fscore_support(y_true = np.array(annotation), y_pred = np.array(model_hypothesis), average='micro')\n","      precision.append(micro_res[0])\n","      recall.append(micro_res[1])\n","      fscore.append(micro_res[2])\n","\n","    # calculate Jaccard similarity between annotation and model hypothesis \n","    jaccard_simm = jaccard_sim(annotation, model_hypothesis)\n","    jacc_similarity.append(jaccard_simm)\n","\n","    # Compute cosine similarity between annotation and model hypothesis \n","    a = getExistingWordsFromModel(annotation)\n","    b = getExistingWordsFromModel(model_hypothesis)\n","    \n","    if len(a) > 0 and len(b) > 0:\n","      cos_sim = word_vectors.n_similarity(a, b)\n","      cos_similarity.append(cos_sim)\n","\n","  return BLEU_scores, jacc_similarity, cos_similarity, precision, recall, fscore"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvefVXbotTou","colab_type":"text"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"luvSLHOWt2A2","colab_type":"text"},"source":["## Run Experiments for LDA_topic distribution based expertise on SO data, using SO_past model"]},{"cell_type":"code","metadata":{"id":"IP5yARVMs48e","colab_type":"code","colab":{}},"source":["def main_topicDistr(lda_model, topic_info):\n","  threshold = [0.10, 0.12, 0.14, 0.16, 0.18, 0.20,\n","             0.22, 0.24, 0.26, 0.28, 0.30,\n","             0.32, 0.34, 0.36, 0.38, 0.40,\n","             0.42, 0.44, 0.46, 0.48, 0.50]\n","  mean_jaccard = []\n","  mean_bleu = []\n","  mean_cos = []\n","  mean_fscore = []\n","\n","  for t in threshold:\n","    BLEU_scores, jacc_sim, cos_sim, precision, recall, fscore = evaluate_LDA_topicDistr(lda_model, topic_info, t)\n","    bleu_np = np.asarray(BLEU_scores)\n","    jacc_np = np.asarray(jacc_sim)\n","    cos_np = np.asarray(cos_sim)\n","    fscore_np = np.asarray(fscore)\n","\n","    mean_jaccard.append( np.mean(jacc_np) )\n","    mean_bleu.append( np.mean(bleu_np) )\n","    mean_cos.append( np.mean(cos_np) )\n","    mean_fscore.append( np.mean(fscore_np) )\n","  return np.max( np.asarray(mean_bleu) ), np.max( np.asarray(mean_jaccard) ), np.max( np.asarray(mean_cos) ), np.max( np.asarray(mean_fscore) )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPtd5-OpEqR0","colab_type":"text"},"source":["## Run Experiments for LDA_topicEmbedding using Avg-pooling on SO data, using SO_full model"]},{"cell_type":"code","metadata":{"id":"pbjPr8PdlGsc","colab_type":"code","colab":{}},"source":["def main_LDA_avgEmb(lda_model, topic_info, term_emb):\n","  threshold_values = [0.40, 0.42, 0.44, 0.46, 0.48, 0.50, 0.52, 0.54, 0.56, \n","                    0.58, 0.60, 0.62, 0.64, 0.66, 0.68, 0.70, 0.72, 0.74, \n","                    0.76, 0.78, 0.80, 0.82, 0.84, 0.86, 0.88, 0.90]\n","  mean_jaccard = []\n","  mean_bleu = []\n","  mean_cos = []\n","  mean_fscore = []\n","  \n","  for threshold in threshold_values:\n","    BLEU_scores, jacc_sim, cos_sim, precision, recall, fscore = evaluate_LDA_topicEmb(lda_model, topic_info, term_emb, threshold, maxPool=False)\n","    bleu_np = np.asarray(BLEU_scores)\n","    jacc_np = np.asarray(jacc_sim)\n","    cos_np = np.asarray(cos_sim)\n","    fscore_np = np.asarray(fscore)\n","\n","    mean_jaccard.append( np.mean(jacc_np) )\n","    mean_bleu.append( np.mean(bleu_np) )\n","    mean_cos.append( np.mean(cos_np) )\n","    mean_fscore.append( np.mean(fscore_np) )\n","  return np.max( np.asarray(mean_bleu) ), np.max( np.asarray(mean_jaccard) ), np.max( np.asarray(mean_cos) ), np.max( np.asarray(mean_fscore) )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PGPrtsxMEzXd","colab_type":"text"},"source":["## Run Experiments for LDA_topicEmbedding using Max-pooling on SO data, using SO_past model"]},{"cell_type":"code","metadata":{"id":"C8h3HVNXGKvp","colab_type":"code","colab":{}},"source":["def main_LDA_maxEmb(lda_model, topic_info, term_emb):\n","  threshold_values = [0.20, 0.25, 0.30, 0.35, 0.40, 0.42, 0.44, 0.46, 0.48, \n","                      0.50, 0.52, 0.54, 0.56, 0.58, 0.60, 0.62, 0.64, 0.66,\n","                      0.68, 0.70, 0.72, 0.74, 0.76, 0.78, 0.80, 0.82, 0.84, 0.86, 0.88, 0.90] \n","  mean_jaccard = []\n","  mean_bleu = []\n","  mean_cos = []\n","  mean_fscore = []\n","\n","  for threshold in threshold_values:\n","    BLEU_scores, jacc_sim, cos_sim, precision, recall, fscore = evaluate_LDA_topicEmb(lda_model, topic_info, term_emb, threshold, maxPool=True)\n","    bleu_np = np.asarray(BLEU_scores)\n","    jacc_np = np.asarray(jacc_sim)\n","    cos_np = np.asarray(cos_sim)\n","    fscore_np = np.asarray(fscore)\n","\n","    mean_jaccard.append( np.mean(jacc_np) )\n","    mean_bleu.append( np.mean(bleu_np) )\n","    mean_cos.append( np.mean(cos_np) )\n","    mean_fscore.append( np.mean(fscore_np) )\n","  return np.max( np.asarray(mean_bleu) ), np.max( np.asarray(mean_jaccard) ), np.max( np.asarray(mean_cos) ), np.max( np.asarray(mean_fscore) )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"El_anO6gkt9U","colab_type":"text"},"source":["##Run Experiments for Word2vec user and topic Embedding using Avg-pooling on SO data"]},{"cell_type":"code","metadata":{"id":"uwfk2yrNkwUx","colab_type":"code","colab":{}},"source":["def main_Word2Vec_AvgEmb(lda_model, topic_info):\n","  threshold_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10,\n","                    0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20]\n","  mean_jaccard = []\n","  mean_bleu = []\n","  mean_cos = []\n","  mean_fscore = []\n","\n","  for threshold in threshold_values:\n","    BLEU_scores, jacc_sim, cos_sim, precision, recall, fscore = evaluate_Word2Vec_Emb(lda=lda_model, topicInfo=topic_info, threshold=threshold, maxPool=False)\n","    bleu_np = np.asarray(BLEU_scores)\n","    jacc_np = np.asarray(jacc_sim)\n","    cos_np = np.asarray(cos_sim)\n","    fscore_np = np.asarray(fscore)\n","\n","    mean_jaccard.append( np.mean(jacc_np) )\n","    mean_bleu.append( np.mean(bleu_np) )\n","    mean_cos.append( np.mean(cos_np) )\n","    mean_fscore.append( np.mean(fscore_np) )\n","  return np.max( np.asarray(mean_bleu) ), np.max( np.asarray(mean_jaccard) ), np.max( np.asarray(mean_cos) ), np.max( np.asarray(mean_fscore) )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzZjW4yhkzCB","colab_type":"text"},"source":["## Run Experiments for Word2vec user and topic Embedding using Max-pooling on SO data"]},{"cell_type":"code","metadata":{"id":"ypVPnK51kynS","colab_type":"code","colab":{}},"source":["def main_Word2Vec_MaxEmb(lda_model, topic_info):\n","  threshold_values = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10,\n","                    0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20,\n","                    0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30,\n","                    0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.40]\n","  mean_jaccard = []\n","  mean_bleu = []\n","  mean_cos = []\n","  mean_fscore = []\n","\n","  for threshold in threshold_values:\n","    BLEU_scores, jacc_sim, cos_sim, precision, recall, fscore = evaluate_Word2Vec_Emb(lda_model, topicInfo=topic_info, threshold = threshold, maxPool=True)\n","    bleu_np = np.asarray(BLEU_scores)\n","    jacc_np = np.asarray(jacc_sim)\n","    cos_np = np.asarray(cos_sim)\n","    fscore_np = np.asarray(fscore)\n","\n","    mean_jaccard.append( np.mean(jacc_np) )\n","    mean_bleu.append( np.mean(bleu_np) )\n","    mean_cos.append( np.mean(cos_np) )\n","    mean_fscore.append( np.mean(fscore_np) )\n","  return np.max( np.asarray(mean_bleu) ), np.max( np.asarray(mean_jaccard) ), np.max( np.asarray(mean_cos) ), np.max( np.asarray(mean_fscore) )    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4K4TFAPmAegT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":490},"outputId":"e80bbfa2-d28b-4750-e519-642b06266dc4","executionInfo":{"status":"error","timestamp":1576717010361,"user_tz":300,"elapsed":1356534,"user":{"displayName":"utherpendragon17@yahoo.com","photoUrl":"","userId":"00837845241358288631"}}},"source":["H_IDs, SO_IDs, GH_annotation_intersect, GH_annotation_union, SO_annotation_intersect, SO_annotation_union = load_annotations()\n","path = '/content/drive/My Drive/sample_data/'\n","runs = 10\n","\n","dictionary = Dictionary.load(path + 'SO_full_processed_Dictionary.dict')\n","corpus = MmCorpus(datapath(path + 'corpus_processed_SO_full.mm'))\n","\n","texts = []\n","with open(path + 'new_SO_full_processed_corpus.csv', 'r') as f:\n","    reader = csv.reader(f)\n","    texts = list(reader)\n","\n","terms = []\n","for (key, value) in dictionary.iteritems():\n","  terms.append(value)\n","\n","def write_results_to_file(path, lda_model, max_bleu, max_jaccard, max_cos, max_fscore):\n","  with open(path, 'a') as f:\n","    writer = csv.writer(f, delimiter = ',', quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n","    writer.writerow([str(lda_model.num_topics), str(lda_model.eta), str(max_bleu), str(max_jaccard), str(max_cos), str(max_fscore)])\n","\n","\n","def saveModelConfigs(model, coherence, u_mass, c_uci, c_npmi, path):\n","  with open(path, 'a') as f:\n","    writer = csv.writer(f, delimiter = ',', quotechar='\"', quoting = csv.QUOTE_MINIMAL)\n","    writer.writerow([str(model.num_topics), str(model.eta), str(coherence), str(u_mass), str(c_uci), str(c_npmi)])\n","\n","\n","def evaluateModel(lda_model, topic_info, term_emb, mode):\n","  if mode == 1:\n","    max_bleu, max_jaccard, max_cos, max_fscore = main_topicDistr(lda_model, topic_info)\n","  elif mode == 2:\n","    max_bleu, max_jaccard, max_cos, max_fscore = main_LDA_avgEmb(lda_model, topic_info, term_emb)\n","  elif mode == 3:\n","    max_bleu, max_jaccard, max_cos, max_fscore = main_LDA_maxEmb(lda_model, topic_info, term_emb)\n","  elif mode == 4:\n","    max_bleu, max_jaccard, max_cos, max_fscore = main_Word2Vec_AvgEmb(lda_model, topic_info)\n","  elif mode == 5:\n","    max_bleu, max_jaccard, max_cos, max_fscore = main_Word2Vec_MaxEmb(lda_model, topic_info)\n","  #write_results_to_file(path, lda_model, max_bleu, max_jaccard, max_cos, max_fscore)\n","  return max_cos\n","\n","\n","model = LdaTransformer(id2word=dictionary, alpha='auto', iterations=100, random_state=2019)\n","\n","# The list of hyper-parameters we want to optimize\n","space = [skopt.space.Integer(10, 100, name='num_topics'), skopt.space.Real(0.001, 1, name='eta')]\n","\n","@use_named_args(space)\n","def objective(**params):\n","  model.set_params(**params)\n","  lda = model.fit(corpus)\n","  term_topic_matrix = lda.gensim_model.get_topics()\n","\n","  term_emb = pd.DataFrame(term_topic_matrix, columns=terms)\n","  topic_info = get_topic_info(lda.gensim_model, corpus, dictionary)\n","\n","  cosine = evaluateModel(lda.gensim_model, topic_info, term_emb, mode = 1)\n","  return 1 - cosine  # to maximize cosine, minimize 1 - cosine\n","\n","res_gp = gp_minimize(func=objective, dimensions=space, n_calls=runs, n_restarts_optimizer= 5,\n","                      n_random_starts=5, random_state=2019, verbose=True)\n","\n","\n","#try:\n","#    dump(res_gp, '/home/norberteke/PycharmProjects/Thesis/data/SO_past_param_optimizer_result_10000.pkl')\n","#except:\n","#    dump(res_gp,\n","#             '/home/norberteke/PycharmProjects/Thesis/data/SO_past_param_optimizer_result_without_objective_10000.pkl', store_objective=False)\n","\"\"\"\n","finally:\n","    try:\n","        print(\"Best score=%.6f\" % (1 - res_gp.fun))  # to make up for 1 - coherence (reverse the equation)\n","\n","        print(\"Best parameters:\n","            - num_topics=%d\n","            - eta=%.6f\" % (res_gp.x[0], res_gp.x[1]))\n","    except:\n","        print(\"Problems occured\")\n","        print(str(res_gp.fun))\n","        print(str(res_gp.x[0]), str(res_gp.x[1]))\n","\n","from skopt.plots import plot_convergence\n","plot_convergence(res_gp)\n","\"\"\""],"execution_count":55,"outputs":[{"output_type":"stream","text":["Iteration No: 1 started. Evaluating function at random point.\n","Iteration No: 1 ended. Evaluation done at random point.\n","Time taken: 1092.4396\n","Function value obtained: 0.7006\n","Current minimum: 0.7006\n","Iteration No: 2 started. Evaluating function at random point.\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-bc20c49b4fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m res_gp = gp_minimize(func=objective, dimensions=space, n_calls=runs, n_restarts_optimizer= 5,\n\u001b[0;32m---> 62\u001b[0;31m                       n_random_starts=5, random_state=2019, verbose=True)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-bc20c49b4fa1>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mterm_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mtopic_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-1da29a84f3f5>\u001b[0m in \u001b[0;36mget_topic_info\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyLDAvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmy_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-1da29a84f3f5>\u001b[0m in \u001b[0;36mmy_prepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, n_jobs, sort_topics)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0;32m--> 184\u001b[0;31m                              term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtopic_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-1da29a84f3f5>\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[1;32m     98\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[0;32m---> 99\u001b[0;31m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}